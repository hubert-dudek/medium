{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b435850-e28b-4dbc-b979-773fa02c10b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "system.data_classification.results + updated UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de356953-9adc-4bdd-a82b-14aec4139def",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "mlflow system tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d352f9-291b-4cfa-a870-9585dc3c4c8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook cell â€” minimal experiment + UC model\n",
    "\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# --- 0) Configure MLflow endpoints\n",
    "mlflow.set_tracking_uri(\"databricks\")      # tracking in this workspace\n",
    "mlflow.set_registry_uri(\"databricks-uc\")   # register models in Unity Catalog (UC)  ðŸ”‘\n",
    "# (UC model registry is recommended; the URI above is the standard way to target it.)  # noqa\n",
    "# Ref: Databricks \"Manage model lifecycle in Unity Catalog\". :contentReference[oaicite:1]{index=1}\n",
    "\n",
    "# --- 1) Create (or set) an MLflow experiment under your /Users/<you>/ folder\n",
    "user = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "EXP_NAME = f\"/Users/{user}/uc_minimal_exp\"\n",
    "\n",
    "# OPTIONAL: store experiment artifacts in a UC Volume (requires MLflow â‰¥ 2.15)\n",
    "# CATALOG, SCHEMA, VOLUME = \"hub\", \"default\", \"mlops_vol\"  # adjust if you have a volume\n",
    "# ARTIFACT_PATH = f\"dbfs:/Volumes/{CATALOG}/{SCHEMA}/{VOLUME}/experiments/{user}/uc_minimal_exp\"\n",
    "# if mlflow.get_experiment_by_name(EXP_NAME) is None:\n",
    "#     mlflow.create_experiment(name=EXP_NAME, artifact_location=ARTIFACT_PATH)  # UC Volumes path\n",
    "# else:\n",
    "#     pass\n",
    "# If you don't use a UC Volume, the default DBFS artifact store is fine for a minimal demo.  # noqa\n",
    "# Details on using UC Volumes for experiment artifacts. :contentReference[oaicite:2]{index=2}\n",
    "\n",
    "if mlflow.get_experiment_by_name(EXP_NAME) is None:\n",
    "    mlflow.create_experiment(name=EXP_NAME)  # simplest: default DBFS artifact store\n",
    "mlflow.set_experiment(EXP_NAME)\n",
    "\n",
    "# --- 2) Simplest possible Spark DataFrame\n",
    "df = spark.createDataFrame([(0.0,), (1.0,), (2.0,), (3.0,)], [\"x\"])\n",
    "\n",
    "# --- 3) Simplest possible MLflow model: doubles 'x'\n",
    "class Doubler(mlflow.pyfunc.PythonModel):\n",
    "    def predict(self, context, model_input: pd.DataFrame):\n",
    "        return model_input[\"x\"] * 2\n",
    "\n",
    "# --- 4) Track a run + log & register the model in UC (hub.default.doubler)\n",
    "model_name = \"hub.default.doubler\"  # <catalog>.<schema>.<registered_model_name>\n",
    "\n",
    "with mlflow.start_run(run_name=\"doubler_minimal\") as run:\n",
    "    # a couple of lightweight tracking calls\n",
    "    mlflow.log_param(\"model_type\", \"pyfunc_doubler\")\n",
    "    mlflow.log_metric(\"sanity_in\", 1.0)\n",
    "    mlflow.log_metric(\"sanity_out\", 2.0)\n",
    "\n",
    "    input_example = pd.DataFrame({\"x\": [1.0]})\n",
    "    signature = infer_signature(input_example, pd.Series([2.0]))\n",
    "\n",
    "    mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=Doubler(),\n",
    "        registered_model_name=model_name,   # registers in UC\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "    )\n",
    "\n",
    "    print(f\"Experiment path: {mlflow.get_experiment(run.info.experiment_id).name}\")\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n",
    "\n",
    "# --- 5) Resolve the latest UC model version, then score the Spark DataFrame\n",
    "client = MlflowClient()\n",
    "versions = [int(v.version) for v in client.search_model_versions(f\"name = '{model_name}'\")]\n",
    "latest_version = max(versions)\n",
    "model_uri = f\"models:/{model_name}/{latest_version}\"\n",
    "print(f\"Registered in UC as: {model_uri}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05 system tables",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
