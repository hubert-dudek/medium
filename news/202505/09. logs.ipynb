{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9c7bb6f-4517-4ba6-a48c-9319b4ba58f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# 1) Imports and setup\n",
    "# -------------------------------------------------------------------\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    input_file_name, monotonically_increasing_id, to_timestamp,\n",
    "    sum as _sum, when, lit\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "from pyspark.sql.window import Window\n",
    "import re\n",
    "\n",
    "# Path to the folder containing log4j .log files\n",
    "logs_path = \"/Volumes/eastus/default/logs/0425-120643-4kmoay69/driver/*.log\"\n",
    "\n",
    "# Compile a timestamp pattern for lines that look like 'dd/MM/yy HH:mm:ss'\n",
    "log_entry_pattern = re.compile(r'^\\d{2}/\\d{2}/\\d{2} \\d{2}:\\d{2}:\\d{2}')\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Read the log lines and tag lines starting a new log entry\n",
    "# -------------------------------------------------------------------\n",
    "# Read lines from log files\n",
    "df_raw = (\n",
    "    spark.read.text(logs_path)\n",
    "         .withColumn(\"filename\", input_file_name())\n",
    "         .withColumn(\"line_id\", monotonically_increasing_id())\n",
    ")\n",
    "\n",
    "# Define UDF to detect lines starting with a timestamp\n",
    "@F.udf(returnType=BooleanType())\n",
    "def starts_with_timestamp(line):\n",
    "    return bool(log_entry_pattern.match(line.strip() if line else \"\"))\n",
    "\n",
    "# Tag each line: 1 if starts a new entry, else 0\n",
    "df_tagged = df_raw.withColumn(\n",
    "    \"is_new\",\n",
    "    when(starts_with_timestamp(F.col(\"value\")), lit(1)).otherwise(lit(0))\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) Create cumulative group IDs based on new entry detection\n",
    "# -------------------------------------------------------------------\n",
    "# Use a window to cumulative sum is_new flags\n",
    "window_spec = (\n",
    "    Window.partitionBy(\"filename\")\n",
    "          .orderBy(\"line_id\")\n",
    "          .rowsBetween(Window.unboundedPreceding, 0)\n",
    ")\n",
    "\n",
    "df_grouped = df_tagged.withColumn(\n",
    "    \"group_id\",\n",
    "    _sum(\"is_new\").over(window_spec)\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Group by 'filename' and 'group_id' and collect lines into arrays\n",
    "# -------------------------------------------------------------------\n",
    "df_collected = (\n",
    "    df_grouped\n",
    "    .groupBy(\"filename\", \"group_id\")\n",
    "    .agg(F.collect_list(\"value\").alias(\"lines\"))\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 5) Define UDF to parse collected lines into timestamp, log level, message\n",
    "# -------------------------------------------------------------------\n",
    "parse_schema = StructType([\n",
    "    StructField(\"timestamp_str\", StringType()),\n",
    "    StructField(\"log_level\",     StringType()),\n",
    "    StructField(\"message\",       StringType()),\n",
    "])\n",
    "\n",
    "@F.udf(returnType=parse_schema)\n",
    "def parse_log_entry(lines):\n",
    "    if not lines:\n",
    "        return (\"\", \"\", \"\")\n",
    "\n",
    "    first_line = lines[0].strip()\n",
    "    parts = first_line.split(maxsplit=3)\n",
    "\n",
    "    if len(parts) >= 3:\n",
    "        timestamp_str = parts[0] + \" \" + parts[1]\n",
    "        log_level     = parts[2]\n",
    "        message       = parts[3] if len(parts) > 3 else \"\"\n",
    "    else:\n",
    "        # If parsing fails, treat all lines as message\n",
    "        return (\"\", \"\", \"\\n\".join(lines))\n",
    "\n",
    "    # Append continuation lines if any\n",
    "    if len(lines) > 1:\n",
    "        message += \"\\n\" + \"\\n\".join(lines[1:])\n",
    "\n",
    "    return (timestamp_str, log_level, message)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 6) Apply parsing function to grouped lines\n",
    "# -------------------------------------------------------------------\n",
    "df_parsed = df_collected.withColumn(\n",
    "    \"parsed\",\n",
    "    parse_log_entry(F.col(\"lines\"))\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 7) Extract final columns and convert timestamp\n",
    "# -------------------------------------------------------------------\n",
    "final_df = (\n",
    "    df_parsed\n",
    "    .select(\n",
    "        \"filename\",\n",
    "        \"group_id\",\n",
    "        F.col(\"parsed.timestamp_str\").alias(\"timestamp_str\"),\n",
    "        F.col(\"parsed.log_level\").alias(\"log_level\"),\n",
    "        F.col(\"parsed.message\").alias(\"message\")\n",
    "    )\n",
    "    .withColumn(\"timestamp\", to_timestamp(\"timestamp_str\", \"dd/MM/yy HH:mm:ss\"))\n",
    "    .orderBy(\"filename\", \"group_id\")\n",
    "    .select(\"timestamp\", \"log_level\", \"message\")\n",
    "    \n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 8) Show the final structured logs\n",
    "# -------------------------------------------------------------------\n",
    "display(final_df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "09. logs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
