{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c40b45e-dc7f-44d6-8df3-b2af892d50f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pyspark.sql.functions as F\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 0.  TEST RUNTIME SETTINGS\n",
    "# ------------------------------------------------------------------\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)   # force shuffle joins\n",
    "spark.conf.set(\"spark.databricks.optimizer.dynamicFilePruning\", \"true\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  BUILD WIDE, COMPLEX, SKEWED DATAFRAMES\n",
    "#      – many columns\n",
    "#      – nested/array column\n",
    "#      – heavy computed column\n",
    "#      – 90 % skew on tenant_id\n",
    "# ------------------------------------------------------------------\n",
    "NUM_SOURCE_ROWS  = 100_000_000\n",
    "NUM_TARGET_ROWS  = 200_000_000           # 50 M overlap + 150 M new\n",
    "SKEW_FRACTION    = 0.90                  # 90 % rows get same tenant_id\n",
    "\n",
    "def build_df(start: int, end: int):\n",
    "    df = spark.range(start, end).repartition(512)            # plenty of partitions\n",
    "    df = (df\n",
    "          .withColumn(\"date\", F.expr(\"current_date() - cast(rand()*100 as int)\"))\n",
    "          .withColumn(\"status\", F.when(F.rand() > 0.5, \"active\").otherwise(\"inactive\"))\n",
    "          .withColumn(\"tenant_id\",\n",
    "                      F.when(F.rand() < SKEW_FRACTION, F.lit(\"TenantA\"))\n",
    "                       .otherwise(F.concat(F.lit(\"Tenant\"), F.expr(\"cast(rand()*1000 as int)\"))))\n",
    "          # --- width boosters ---\n",
    "          .withColumn(\"value1\",  F.rand()*1000)\n",
    "          .withColumn(\"value2\",  F.col(\"id\")*5 + F.lit(123))\n",
    "          .withColumn(\"status_flag\", F.when(F.col(\"status\") == \"active\", 1).otherwise(0))\n",
    "          .withColumn(\"id_str\",  F.concat(F.lit(\"ID-\"), F.col(\"id\")))\n",
    "          .withColumn(\"address\",\n",
    "                      F.struct(F.lit(\"123 Main St\").alias(\"street\"),\n",
    "                               F.lit(\"Metropolis\").alias(\"city\")))\n",
    "          .withColumn(\"random_values\", F.array(F.rand(), F.rand(), F.rand()))\n",
    "          .withColumn(\"complex_calc\",\n",
    "                      F.pow(F.col(\"id\").cast(\"double\"), 2) * F.log(F.col(\"id\")+1))\n",
    "          .withColumn(\"last_updated\", F.lit(None).cast(\"timestamp\"))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "source_df  = build_df(1, NUM_SOURCE_ROWS + 1)\n",
    "target_df  = build_df(50_000_000, 50_000_000 + NUM_TARGET_ROWS)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  (RE)CREATE TARGET TABLES\n",
    "# ------------------------------------------------------------------\n",
    "for tbl in (\"target1\", \"target2\", \"target3\"):\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {tbl}\")\n",
    "    target_df.write.mode(\"overwrite\").saveAsTable(tbl)\n",
    "    spark.sql(f\"OPTIMIZE {tbl}\") \n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  DEFINE A HELPER TO TIME MERGES CLEANLY\n",
    "# ------------------------------------------------------------------\n",
    "def time_it(label, fn):\n",
    "    start = time.perf_counter()\n",
    "    fn()\n",
    "    elapsed = time.perf_counter() - start\n",
    "    print(f\"{label:<12s}  {elapsed:,.1f}  seconds\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  MERGE PATTERN 1  – DeltaTable API\n",
    "# ------------------------------------------------------------------\n",
    "def merge_pattern_1():\n",
    "    src = build_df(1, NUM_SOURCE_ROWS + 1)               # fresh source (avoid cache)\n",
    "    DeltaTable.forName(spark, \"target1\") \\\n",
    "      .alias(\"t\") \\\n",
    "      .merge(src.alias(\"s\"), \"t.id = s.id\") \\\n",
    "      .whenMatchedUpdate(set = {\n",
    "          # heavy conditional update to stress CPU\n",
    "          \"status\": \"CASE WHEN t.status = 'inactive' AND s.status = 'active' \"\n",
    "                    \"THEN 'reactivated' ELSE t.status END\",\n",
    "          \"value1\": \"t.value1 + s.value1\",\n",
    "          \"status_flag\": \"CASE WHEN s.status = 'active' THEN 1 ELSE 0 END\",\n",
    "          \"last_updated\": \"current_timestamp()\"\n",
    "      }) \\\n",
    "      .whenNotMatchedInsertAll() \\\n",
    "      .execute()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  MERGE PATTERN 2  – DataFrameWriter.mergeInto\n",
    "# ------------------------------------------------------------------\n",
    "def merge_pattern_2():\n",
    "    src = build_df(1, NUM_SOURCE_ROWS + 1)\n",
    "    (src.alias(\"s\")\n",
    "        .mergeInto(\"target2\", F.expr(\"target2.id = s.id\"))\n",
    "        .whenMatched()\n",
    "            .update({\n",
    "                \"status\":      F.expr(\"CASE WHEN target2.status = 'inactive' \"\n",
    "                                      \"AND s.status = 'active' \"\n",
    "                                      \"THEN 'reactivated' ELSE target2.status END\"),\n",
    "                \"value1\":      F.expr(\"target2.value1 + s.value1\"),\n",
    "                \"status_flag\": F.when(src.status == \"active\", 1).otherwise(0),\n",
    "                \"last_updated\": F.current_timestamp()\n",
    "            })\n",
    "        .whenNotMatched()\n",
    "            .insertAll()\n",
    "        .merge())\n",
    "    \n",
    "# ------------------------------------------------------------------\n",
    "# 6.  MERGE PATTERN 3  – SQL in PySpark\n",
    "# ------------------------------------------------------------------\n",
    "def merge_pattern_3():\n",
    "    src = build_df(1, NUM_SOURCE_ROWS + 1)\n",
    "    spark.sql(\"\"\"\n",
    "        MERGE INTO target3 t\n",
    "        USING {src_df} s\n",
    "        ON   t.id = s.id\n",
    "        WHEN MATCHED THEN UPDATE SET\n",
    "            status       = CASE WHEN t.status = 'inactive' AND s.status = 'active'\n",
    "                                THEN 'reactivated' ELSE t.status END,\n",
    "            value1       = t.value1 + s.value1,\n",
    "            status_flag  = CASE WHEN s.status = 'active' THEN 1 ELSE 0 END,\n",
    "            last_updated = current_timestamp()\n",
    "        WHEN NOT MATCHED THEN\n",
    "            INSERT *\n",
    "    \"\"\", src_df=src)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9b88288-cb2c-48d1-a876-9bef6a5d3391",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "time_it(\"DeltaTable\",   merge_pattern_1) # 148.6  seconds\n",
    "time_it(\"mergeInto\",    merge_pattern_2) # 138.6  seconds\n",
    "time_it(\"SQL MERGE\",    merge_pattern_3) # 147.6  seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0398255-3cf8-4c67-8a53-e643cabc3ee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cluster restart and recreate target tables (1st cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2864a9af-09a6-43ff-9a84-f48906aca13d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "time_it(\"SQL MERGE\",    merge_pattern_3) # 139.0  seconds\n",
    "time_it(\"mergeInto\",    merge_pattern_2) # 144.6  seconds\n",
    "time_it(\"DeltaTable\",   merge_pattern_1) # 153.2  seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c99a8bcd-ebcb-4bdb-a242-8511f13a3d16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cluster restart and recreate target tables (1st cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c195bf8-545d-444e-85a4-7f9074fae1a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "time_it(\"SQL MERGE\",    merge_pattern_3) # 157.8  seconds\n",
    "time_it(\"DeltaTable\",   merge_pattern_1) # 160.9  seconds\n",
    "time_it(\"mergeInto\",    merge_pattern_2) # 146.2  seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a1f5fba-7342-45be-8171-65307d9a96c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cluster restart and recreate target tables (1st cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83653f32-0937-4913-870f-9f3adf4c32a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "time_it(\"mergeInto\",    merge_pattern_2) # 151.3  seconds\n",
    "time_it(\"SQL MERGE\",    merge_pattern_3) # 159.3  seconds\n",
    "time_it(\"DeltaTable\",   merge_pattern_1) # 148.3  seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "127d57ca-4d62-4627-8a4f-0c1afaf3b1cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cluster restart and recreate target tables (1st cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21f14085-87cc-48bc-9e90-eb9da13d4061",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "time_it(\"mergeInto\",    merge_pattern_2) # 147.0  seconds\n",
    "time_it(\"DeltaTable\",   merge_pattern_1) # 154.3  seconds\n",
    "time_it(\"SQL MERGE\",    merge_pattern_3) # 142.8  seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d30a4682-ca6a-483a-860d-6c0c6a9cf520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Cluster restart and recreate target tables (1st cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08f747ef-7517-4ae3-adab-f80716f661af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "time_it(\"DeltaTable\",   merge_pattern_1) # 151.6  seconds\n",
    "time_it(\"SQL MERGE\",    merge_pattern_3) #\n",
    "time_it(\"mergeInto\",    merge_pattern_2) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66518bad-797b-4d30-ab7a-e4ac35057b95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, rand, when, col\n",
    "\n",
    "# Create first DataFrame with random date and status\n",
    "df_source = (spark.range(1, 1_000_000)\n",
    "       .withColumn(\"date\", expr(\"current_date() - CAST(rand() * 100 AS INT)\"))\n",
    "       .withColumn(\"status\", when(rand() > 0.5, \"active\").otherwise(\"inactive\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8739f13-fa41-4a40-abee-37c8a483f64f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create second DataFrame with different IDs and random data\n",
    "df2 = (spark.range(1_000_000, 2_000_000)  # overlapping 20 mln records for merge\n",
    "       .withColumn(\"date\", expr(\"current_date() - CAST(rand() * 100 AS INT)\"))\n",
    "       .withColumn(\"status\", when(rand() > 0.5, \"active\").otherwise(\"inactive\")))\n",
    "\n",
    "# Save both DataFrames as Delta tables\n",
    "df2.write.mode(\"overwrite\").saveAsTable(\"target_old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c6c92cb-0601-46bb-9d9f-008b93f83f5c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example 2"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "display(\n",
    "    df_source.alias(\"source\")\n",
    "    .mergeInto(\"target_old\", col(\"target_old.id\") == col(\"source.id\"))\n",
    "    .whenNotMatched()\n",
    "    .insertAll()\n",
    "    .whenMatched()\n",
    "    .updateAll()\n",
    "    .merge()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32fa7b01-7153-4600-955e-5e5c84084c68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example 1"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "deltaTable1 = DeltaTable.forName(spark, \"target1\")\n",
    "\n",
    "(deltaTable1.alias(\"t1\")\n",
    " .merge(\n",
    "    source=df_source.alias(\"t2\"),\n",
    "    condition=\"t1.id = t2.id\"\n",
    ")\n",
    " .whenMatchedUpdateAll()\n",
    " .whenNotMatchedInsertAll()\n",
    " .execute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b288bf0c-544a-4158-966a-b644f4fb71ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f27941a-df1a-4d23-892c-45dcfbee207a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example 3"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO target3 AS t2\n",
    "    USING {df_source} AS s1\n",
    "    ON t2.id = s1.id\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT *\n",
    "\"\"\", df_source=df_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5515395-026a-4913-a9ec-030cbb74328d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6803046506339496,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "MERGE_test",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
